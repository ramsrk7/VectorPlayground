{"docstore/data": {"43d84308-107b-4e14-ad67-4708ed0df562": {"__data__": {"id_": "43d84308-107b-4e14-ad67-4708ed0df562", "embedding": null, "metadata": {"page_number": 0, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com Abstract There are two existing strategies for applying pre-trained language representations to downWe introduce a new language representastream tasks: feature-based and ne-tuning. The tion model called BERT, which stands for Bidirectional Encoder Representations from feature-based approach, such as ELMo (Peters Transformers. Unlike recent language repreet al., 2018a), uses task-specic architectures that2019 sentation models (Peters et al., 2018a; Radinclude the pre-trained representations as addiford et al., 2018), BERT is designed to pretional features. The ne-tuning approach, such as train deep bidirectional representations from the Generative Pre-trained Transformer (OpenAIMay unlabeled text by jointly conditioning on both GPT) (Radford et al., 2018), introduces minimal left and right context in all layers. As a retask-specic parameters, and is trained on the24 sult, the pre-trained BERT model can be nedownstream tasks by simply ne-tuning all pretuned with just one additional output layer to create state-of-the-art models for a wide trained parameters. The two approaches share the range of tasks, such as question answering and same objective function during pre-training, where language inference, without substantial taskthey use unidirectional language models to learn specic architecture modications. general language representations.[cs.CL] BERT is conceptually simple and empirically We argue that current techniques restrict the powerful. It obtains new state-of-the-art repower of the pre-trained representations, espesults on eleven natural language processing cially for the ne-tuning approaches. The matasks, including pushing the GLUE score to jor limitation is that standard language models are 80.5% (7.7% point absolute improvement), unidirectional, and this limits the choice of archiMultiNLI accuracy to 86.7% (4.6% absolute tectures that can be used during pre-training. For improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute imexample, in OpenAI GPT, the authors use a left-toprovement) and SQuAD v2.0 Test F1 to 83.1 right architecture, where every token can only at- (5.1 point absolute improvement). tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re1 Introduction strictions are sub-optimal for sentence-level tasks, Language model pre-training has been shown to and could be very harmful when applying nebe effective for improving many natural language tuning based approaches to token-level tasks sucharXiv:1810.04805v2 processing tasks (Dai and Le, 2015; Peters et al., as question answering, where it is crucial to incor2018a; Radford et al., 2018; Howard and Ruder, porate context from both directions. 2018). These include sentence-level tasks such as In this paper, we improve the ne-tuning based natural language inference (Bowman et al., 2015; approaches by proposing BERT: Bidirectional Williams et al., 2018) and paraphrasing (Dolan Encoder Representations from Transformers. and Brockett, 2005), which aim to predict the reBERT alleviates the previously mentioned unidilationships between sentences by analyzing them rectionality constraint by using a masked lanholistically, as well as token-level tasks such as guage model (MLM) pre-training objective, innamed entity recognition and question answering, spired by the Cloze task (Taylor, 1953). The where models are required to produce ne-grained masked language model randomly masks some of output at the token level (Tjong Kim Sang and the tokens from the input, and the objective is to", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "0be1ad4b-d510-400f-b008-b9912accfd80": {"__data__": {"id_": "0be1ad4b-d510-400f-b008-b9912accfd80", "embedding": null, "metadata": {"page_number": 1, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "word based only on its context. Unlike left-toThese approaches have been generalized to right language model pre-training, the MLM obcoarser granularities, such as sentence embedjective enables the representation to fuse the left dings (Kiros et al., 2015; Logeswaran and Lee, and the right context, which allows us to pre2018) or paragraph embeddings (Le and Mikolov, train a deep bidirectional Transformer. In addi2014). To train sentence representations, prior tion to the masked language model, we also use work has used objectives to rank candidate next a next sentence prediction task that jointly presentences (Jernite et al., 2017; Logeswaran and trains text-pair representations. The contributions Lee, 2018), left-to-right generation of next senof our paper are as follows: tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising autoWe demonstrate the importance of bidirectional encoder derived objectives (Hill et al., 2016). pre-training for language representations. UnELMo and its predecessor (Peters et al., 2017, like Radford et al. (2018), which uses unidirec2018a) generalize traditional word embedding retional language models for pre-training, BERT search along a different dimension. They extract uses masked language models to enable precontext-sensitive features from a left-to-right and a trained deep bidirectional representations. This right-to-left language model. The contextual repis also in contrast to Peters et al. (2018a), which resentation of each token is the concatenation of uses a shallow concatenation of independently the left-to-right and right-to-left representations. trained left-to-right and right-to-left LMs. When integrating contextual word embeddings We show that pre-trained representations reduce with existing task-specic architectures, ELMo the need for many heavily-engineered taskadvances the state of the art for several major NLP specic architectures. BERT is the rst nebenchmarks (Peters et al., 2018a) including questuning based representation model that achieves tion answering (Rajpurkar et al., 2016), sentiment state-of-the-art performance on a large suite analysis (Socher et al., 2013), and named entity of sentence-level and token-level tasks, outperrecognition (Tjong Kim Sang and De Meulder, forming many task-specic architectures. 2003). Melamud et al. (2016) proposed learning contextual representations through a task to preBERT advances the state of the art for eleven dict a single word from both left and right context NLP tasks. The code and pre-trained modusing LSTMs. Similar to ELMo, their model is els are available at https://github.com/ feature-based and not deeply bidirectional. Fedus google-research/bert. et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod-2 Related Work els. There is a long history of pre-training general language representations, and we briey review the 2.2 Unsupervised Fine-tuning Approaches most widely-used approaches in this section. As with the feature-based approaches, the rst 2.1 Unsupervised Feature-based Approaches works in this direction only pre-trained word emLearning widely applicable representations of bedding parameters from unlabeled text (Colwords has been an active area of research for lobert and Weston, 2008). decades, including non-neural (Brown et al., 1992; More recently, sentence or document encoders Ando and Zhang, 2005; Blitzer et al., 2006) and which produce contextual token representations neural (Mikolov et al., 2013; Pennington et al., have been pre-trained from unlabeled text and 2014) methods. Pre-trained word embeddings ne-tuned for a supervised downstream task (Dai are an integral part of modern NLP systems, ofand Le, 2015; Howard and Ruder, 2018; Radford fering signicant improvements over embeddings et al., 2018). The advantage of these approaches learned from scratch (Turian et al., 2010). To preis that few parameters need to be learned from train word embedding vectors, left-to-right lanscratch. At least partly due to this advantage, guage modeling objectives have been used (Mnih OpenAI GPT (Radford et al., 2018) achieved preand Hinton, 2009), as well as objectives to disviously state-of-the-art results on many sentencecriminate correct from incorrect words in left and level tasks from the GLUE benchmark (Wang", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "5c2eb22f-2436-4825-9018-8c713245228b": {"__data__": {"id_": "5c2eb22f-2436-4825-9018-8c713245228b", "embedding": null, "metadata": {"page_number": 2, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "NSP Mask LM Mask LM MNLI NER SQuAD Start/End Span C T1 ... TN T[SEP] T1 ... TM C T1 ... TN T[SEP] T1 ... TM BERT BERT BERT E[CLS] E1 ... EN E[SEP] E1 ... EM E[CLS] E1 ... EN E[SEP] E1 ... EM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM Masked Sentence A Masked Sentence B Question Paragraph Unlabeled Sentence A and B Pair Question Answer Pair Pre-training Fine-Tuning Figure 1: Overall pre-training and ne-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and ne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During ne-tuning, all parameters are ne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers). ing and auto-encoder objectives have been used mal difference between the pre-trained architecfor pre-training such models (Howard and Ruder, ture and the nal downstream architecture. 2018; Radford et al., 2018; Dai and Le, 2015). Model Architecture BERTs model architec2.3 Transfer Learning from Supervised Data ture is a multi-layer bidirectional Transformer encoder based on the original implementation de-There has also been work showing effective transscribed in Vaswani et al. (2017) and released infer from supervised tasks with large datasets, such the tensor2tensor library.1 Because the useas natural language inference (Conneau et al., of Transformers has become common and our im-2017) and machine translation (McCann et al., plementation is almost identical to the original,2017). Computer vision research has also demonwe will omit an exhaustive background descrip-strated the importance of transfer learning from tion of the model architecture and refer readers tolarge pre-trained models, where an effective recipe Vaswani et al. (2017) as well as excellent guidesis to ne-tune models pre-trained with Imasuch as The Annotated Transformer.2geNet (Deng et al., 2009; Yosinski et al., 2014). In this work, we denote the number of layers 3 BERT (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We introduce BERT and its detailed implementaWe primarily report results on two model sizes: tion in this section. There are two steps in our BERTBASE (L=12, H=768, A=12, Total Paramframework: pre-training and ne-tuning. Dureters=110M) and BERTLARGE (L=24, H=1024, ing pre-training, the model is trained on unlabeled A=16, Total Parameters=340M). data over different pre-training tasks. For neBERTBASE was chosen to have the same model tuning, the BERT model is rst initialized with size as OpenAI GPT for comparison purposes. the pre-trained parameters, and all of the paramCritically, however, the BERT Transformer uses eters are ne-tuned using labeled data from the bidirectional self-attention, while the GPT Transdownstream tasks. Each downstream task has sepformer uses constrained self-attention where every arate ne-tuned models, even though they are initoken can only attend to context to its left.4 tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve 1https://github.com/tensorow/tensor2tensor 2http://nlp.seas.harvard.edu/2018/04/03/attention.htmlas a running example for this section. 3In all cases we set the feed-forward/lter size to be 4H, A distinctive feature of BERT is its unied ari.e., 3072 for the H = 768 and 4096 for the H = 1024.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "2f66606d-1cc9-4021-bd9d-dce8daec50dd": {"__data__": {"id_": "2f66606d-1cc9-4021-bd9d-dce8daec50dd", "embedding": null, "metadata": {"page_number": 3, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Input/Output Representations To make BERT In order to train a deep bidirectional representahandle a variety of down-stream tasks, our input tion, we simply mask some percentage of the input representation is able to unambiguously represent tokens at random, and then predict those masked both a single sentence and a pair of sentences tokens. We refer to this procedure as a masked (e.g., Question, Answer ) in one token sequence. LM (MLM), although it is often referred to as a Throughout this work, a sentence can be an arbiCloze task in the literature (Taylor, 1953). In this trary span of contiguous text, rather than an actual case, the nal hidden vectors corresponding to the linguistic sentence. A sequence refers to the inmask tokens are fed into an output softmax over put token sequence to BERT, which may be a sinthe vocabulary, as in a standard LM. In all of our gle sentence or two sentences packed together. experiments, we mask 15% of all WordPiece toWe use WordPiece embeddings (Wu et al., kens in each sequence at random. In contrast to 2016) with a 30,000 token vocabulary. The rst denoising auto-encoders (Vincent et al., 2008), we token of every sequence is always a special clasonly predict the masked words rather than reconsication token ([CLS]). The nal hidden state structing the entire input. corresponding to this token is used as the agAlthough this allows us to obtain a bidirecgregate sequence representation for classication tional pre-trained model, a downside is that we tasks. Sentence pairs are packed together into a are creating a mismatch between pre-training and single sequence. We differentiate the sentences in ne-tuning, since the [MASK] token does not aptwo ways. First, we separate them with a special pear during ne-tuning. To mitigate this, we do token ([SEP]). Second, we add a learned embednot always replace masked words with the acding to every token indicating whether it belongs tual [MASK] token. The training data generator to sentence A or sentence B. As shown in Figure 1, chooses 15% of the token positions at random for we denote input embedding as E, the nal hidden prediction. If the i-th token is chosen, we replace vector of the special [CLS] token as C RH, the i-th token with (1) the [MASK] token 80% of and the nal hidden vector for the ith input token the time (2) a random token 10% of the time (3) as Ti RH. the unchanged i-th token 10% of the time. Then, For a given token, its input representation is Ti will be used to predict the original token with constructed by summing the corresponding token, cross entropy loss. We compare variations of this segment, and position embeddings. A visualizaprocedure in Appendix C.2. tion of this construction can be seen in Figure 2. Task #2: Next Sentence Prediction (NSP) 3.1 Pre-training BERT Many important downstream tasks such as Question Answering (QA) and Natural Language Infer-Unlike Peters et al. (2018a) and Radford et al. ence (NLI) are based on understanding the rela-(2018), we do not use traditional left-to-right or tionship between two sentences, which is not di-right-to-left language models to pre-train BERT. rectly captured by language modeling. In orderInstead, we pre-train BERT using two unsuperto train a model that understands sentence rela-vised tasks, described in this section. This step tionships, we pre-train for a binarized next sen-is presented in the left part of Figure 1. tence prediction task that can be trivially generTask #1: Masked LM Intuitively, it is reasonated from any monolingual corpus. Specically, able to believe that a deep bidirectional model is when choosing the sentences A and B for each prestrictly more powerful than either a left-to-right training example, 50% of the time B is the actual model or the shallow concatenation of a left-tonext sentence that follows A (labeled as IsNext), right and a right-to-left model. Unfortunately, and 50% of the time it is a random sentence from standard conditional language models can only be the corpus (labeled as NotNext). As we show trained left-to-right or right-to-left, since bidirecin Figure 1, C is used for next sentence predictional conditioning would allow each word to intion (NSP).5 Despite its simplicity, we demondirectly see itself, and the model could trivially strate in Section 5.1 that pre-training towards this predict the target word in a multi-layered context. task is very benecial to both QA and NLI. 6 former is often referred to as a Transformer encoder while 5The nal model achieves 97%-98% accuracy on NSP. the left-context-only version is referred to as a Transformer 6The vector C is not a meaningful sentence representation", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "5d78d132-a90b-439f-889c-e3ad51794a8e": {"__data__": {"id_": "5d78d132-a90b-439f-889c-e3ad51794a8e", "embedding": null, "metadata": {"page_number": 4, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Input [CLS] my dog is cute [SEP] he likes play ##ing [SEP] Token Embeddings E[CLS] Emy Edog Eis Ecute E[SEP] Ehe Elikes Eplay E##ing E[SEP] Segment Embeddings EA EA EA EA EA EA EB EB EB EB EB Position Embeddings E0 E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings. The NSP task is closely related to representation- (4) a degenerate text-pair in text classication learning objectives used in Jernite et al. (2017) and or sequence tagging. At the output, the token repLogeswaran and Lee (2018). However, in prior resentations are fed into an output layer for tokenwork, only sentence embeddings are transferred to level tasks, such as sequence tagging or question down-stream tasks, where BERT transfers all paanswering, and the [CLS] representation is fed rameters to initialize end-task model parameters. into an output layer for classication, such as entailment or sentiment analysis. Pre-training data The pre-training procedure Compared to pre-training, ne-tuning is relalargely follows the existing literature on language tively inexpensive. All of the results in the pamodel pre-training. For the pre-training corpus we per can be replicated in at most 1 hour on a sinuse the BooksCorpus (800M words) (Zhu et al., gle Cloud TPU, or a few hours on a GPU, starting 2015) and English Wikipedia (2,500M words). from the exact same pre-trained model.7 We deFor Wikipedia we extract only the text passages scribe the task-specic details in the correspondand ignore lists, tables, and headers. It is critiing subsections of Section 4. More details can be cal to use a document-level corpus rather than a found in Appendix A.5. shufed sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to 4 Experiments extract long contiguous sequences. In this section, we present BERT ne-tuning re3.2 Fine-tuning BERT sults on 11 NLP tasks. Fine-tuning is straightforward since the self4.1 GLUE attention mechanism in the Transformer alThe General Language Understanding Evaluation lows BERT to model many downstream tasks (GLUE) benchmark (Wang et al., 2018a) is a colwhether they involve single text or text pairsby lection of diverse natural language understanding swapping out the appropriate inputs and outputs. tasks. Detailed descriptions of GLUE datasets are For applications involving text pairs, a common included in Appendix B.1. pattern is to independently encode text pairs beTo ne-tune on GLUE, we represent the input fore applying bidirectional cross attention, such sequence (for single sentence or sentence pairs) as Parikh et al. (2016); Seo et al. (2017). BERT as described in Section 3, and use the nal hidinstead uses the self-attention mechanism to unify den vector C RH corresponding to the rst these two stages, as encoding a concatenated text input token ([CLS]) as the aggregate representapair with self-attention effectively includes bidition. The only new parameters introduced during rectional cross attention between two sentences. ne-tuning are classication layer weights W For each task, we simply plug in the taskRKH, where K is the number of labels. We comspecic inputs and outputs into BERT and nepute a standard classication loss with C and W, tune all the parameters end-to-end. At the inT i.e., log(softmax(CW )). put, sentence A and sentence B from pre-training 7For example, the BERT SQuAD model can be trained inare analogous to (1) sentence pairs in paraphrasaround 30 minutes on a single Cloud TPU to achieve a Dev ing, (2) hypothesis-premise pairs in entailment, (3) F1 score of 91.0%.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "278d4f3b-57f5-4afa-9e1e-3ce662b5687e": {"__data__": {"id_": "278d4f3b-57f5-4afa-9e1e-3ce662b5687e", "embedding": null, "metadata": {"page_number": 5, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average 392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k - Pre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0 BiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0 OpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1 BERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6 BERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1 Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The Average column is slightly different than the ofcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are singlemodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components. We use a batch size of 32 and ne-tune for 3 Wikipedia containing the answer, the task is to epochs over the data for all GLUE tasks. For each predict the answer text span in the passage. task, we selected the best ne-tuning learning rate As shown in Figure 1, in the question answer- (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. ing task, we represent the input question and pasAdditionally, for BERTLARGE we found that nesage as a single packed sequence, with the questuning was sometimes unstable on small datasets, tion using the A embedding and the passage using so we ran several random restarts and selected the the B embedding. We only introduce a start vecbest model on the Dev set. With random restarts, tor S RH and an end vector E RH during we use the same pre-trained checkpoint but perne-tuning. The probability of word i being the form different ne-tuning data shufing and classtart of the answer span is computed as a dot prodsier layer initialization.9 uct between Ti and S followed by a softmax over Results are presented in Table 1. Both all of the words in the paragraph: Pi = eSTi . Pj eSTjBERTBASE and BERTLARGE outperform all sysThe analogous formula is used for the end of the tems on all tasks by a substantial margin, obtaining answer span. The score of a candidate span from 4.5% and 7.0% respective average accuracy imposition i to position j is dened as STi + ETj, provement over the prior state of the art. Note that and the maximum scoring span where j i is BERTBASE and OpenAI GPT are nearly identical used as a prediction. The training objective is the in terms of model architecture apart from the atsum of the log-likelihoods of the correct start and tention masking. For the largest and most widely end positions. We ne-tune for 3 epochs with a reported GLUE task, MNLI, BERT obtains a 4.6% learning rate of 5e-5 and a batch size of 32. absolute accuracy improvement. On the ofcial Table 2 shows top leaderboard entries as well GLUE leaderboard10, BERTLARGE obtains a score as results from top published systems (Seo et al., of 80.5, compared to OpenAI GPT, which obtains 2017; Clark and Gardner, 2018; Peters et al., 72.8 as of the date of writing. 2018a; Hu et al., 2018). The top results from the We nd that BERTLARGE signicantly outperSQuAD leaderboard do not have up-to-date public forms BERTBASE across all tasks, especially those system descriptions available,11 and are allowed to with very little training data. The effect of model use any public data when training their systems. size is explored more thoroughly in Section 5.2. We therefore use modest data augmentation in our system by rst ne-tuning on TriviaQA (Joshi4.2 SQuAD v1.1 et al., 2017) befor ne-tuning on SQuAD. The Stanford Question Answering Dataset Our best performing system outperforms the top (SQuAD v1.1) is a collection of 100k crowdleaderboard system by +1.5 F1 in ensembling and sourced question/answer pairs (Rajpurkar et al., +1.3 F1 as a single system. In fact, our single 2016). Given a question and a passage from BERT model outperforms the top ensemble sys9The GLUE data set distribution does not include the Test tem in terms of F1 score. Without TriviaQA nelabels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE. 11QANet is described in Yu et al. (2018), but the system", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "2109d277-d375-4186-b0ad-18c92b006c39": {"__data__": {"id_": "2109d277-d375-4186-b0ad-18c92b006c39", "embedding": null, "metadata": {"page_number": 6, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "System Dev Test System Dev Test EM F1 EM F1 ESIM+GloVe 51.9 52.7 Top Leaderboard Systems (Dec 10th, 2018) ESIM+ELMo 59.1 59.2 Human - - 82.3 91.2 OpenAI GPT - 78.0 #1 Ensemble - nlnet - - 86.0 91.7 BERTBASE 81.6 - #2 Ensemble - QANet - - 84.5 90.5 BERTLARGE 86.6 86.3 Published Human (expert) - 85.0 BiDAF+ELMo (Single) - 85.6 - 85.8 R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5 Human (5 annotations) - 88.0 Ours Table 4: SWAG Dev and Test accuracies. Human perBERTBASE (Single) 80.8 88.5 - - BERTLARGE (Single) 84.1 90.9 - - formance is measured with 100 samples, as reported in BERTLARGE (Ensemble) 85.8 91.8 - - the SWAG paper. BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8 BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2 si,j = maxjiSTi + ETj. We predict a non-null Table 2: SQuAD 1.1 results. The BERT ensemble answer when si,j > snull + , where the threshis 7x systems which use different pre-training checkold is selected on the dev set to maximize F1.points and ne-tuning seeds. We did not use TriviaQA data for this model. We ne-tuned for 2 epochs with a learning rate of 5e-5 System Dev Test EM F1 EM F1 and a batch size of 48. Top Leaderboard Systems (Dec 10th, 2018) The results compared to prior leaderboard enHuman 86.3 89.0 86.9 89.5 tries and top published work (Sun et al., 2018; #1 Single - MIR-MRC (F-Net) - - 74.8 78.0 Wang et al., 2018b) are shown in Table 3, exclud- #2 Single - nlnet - - 74.2 77.1 ing systems that use BERT as one of their comPublished ponents. We observe a +5.1 F1 improvement over unet (Ensemble) - - 71.4 74.9 SLQA+ (Single) - 71.4 74.4 the previous best system. Ours BERTLARGE (Single) 78.7 81.9 80.0 83.1 4.4 SWAG The Situations With Adversarial Generations Table 3: SQuAD 2.0 results. We exclude entries that (SWAG) dataset contains 113k sentence-pair comuse BERT as one of their components. pletion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible con-tuning data, we only lose 0.1-0.4 F1, still outpertinuation among four choices.forming all existing systems by a wide margin.12 When ne-tuning on the SWAG dataset, we 4.3 SQuAD v2.0 construct four input sequences, each containing the concatenation of the given sentence (sentence The SQuAD 2.0 task extends the SQuAD 1.1 A) and a possible continuation (sentence B). The problem denition by allowing for the possibility only task-specic parameters introduced is a vecthat no short answer exists in the provided parator whose dot product with the [CLS] token repgraph, making the problem more realistic. resentation C denotes a score for each choice We use a simple approach to extend the SQuAD which is normalized with a softmax layer. v1.1 BERT model for this task. We treat quesWe ne-tune the model for 3 epochs with a tions that do not have an answer as having an anlearning rate of 2e-5 and a batch size of 16. Reswer span with start and end at the [CLS] tosults are presented in Table 4. BERTLARGE outken. The probability space for the start and end performs the authors baseline ESIM+ELMo sysanswer span positions is extended to include the tem by +27.1% and OpenAI GPT by 8.3%. position of the [CLS] token. For prediction, we compare the score of the no-answer span: snull = 5 Ablation Studies SC + EC to the score of the best non-null span In this section, we perform ablation experiments 12The TriviaQA data we used consists of paragraphs from over a number of facets of BERT in order to betterTriviaQA-Wiki formed of the rst 400 tokens in documents,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "6882fc02-aded-461b-8609-0de660a75dc9": {"__data__": {"id_": "6882fc02-aded-461b-8609-0de660a75dc9", "embedding": null, "metadata": {"page_number": 7, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Dev Set results are still far worse than those of the preTasks MNLI-m QNLI MRPC SST-2 SQuAD trained bidirectional models. The BiLSTM hurts (Acc) (Acc) (Acc) (Acc) (F1) performance on the GLUE tasks. BERTBASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 We recognize that it would also be possible to LTR & No NSP 82.1 84.3 77.5 92.1 77.8 train separate LTR and RTL models and represent + BiLSTM 82.1 84.1 75.7 91.6 84.9 each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as Table 5: Ablation over the pre-training tasks using the expensive as a single bidirectional model; (b) this BERTBASE architecture. No NSP is trained without is non-intuitive for tasks like QA, since the RTLthe next sentence prediction task. LTR & No NSP is trained as a left-to-right LM without the next sentence model would not be able to condition the answer prediction, like OpenAI GPT. + BiLSTM adds a ranon the question; (c) this it is strictly less powerful domly initialized BiLSTM on top of the LTR + No than a deep bidirectional model, since it can use NSP model during ne-tuning. both left and right context at every layer. 5.2 Effect of Model Size ablation studies can be found in Appendix C. In this section, we explore the effect of model size 5.1 Effect of Pre-training Tasks on ne-tuning task accuracy. We trained a number of BERT models with a differing number of layers, We demonstrate the importance of the deep bidihidden units, and attention heads, while otherwise rectionality of BERT by evaluating two preusing the same hyperparameters and training protraining objectives using exactly the same precedure as described previously. training data, ne-tuning scheme, and hyperpaResults on selected GLUE tasks are shown inrameters as BERTBASE: Table 6. In this table, we report the average Dev No NSP: A bidirectional model which is trained Set accuracy from 5 random restarts of ne-tuning. using the masked LM (MLM) but without the We can see that larger models lead to a strict acnext sentence prediction (NSP) task. curacy improvement across all four datasets, even LTR & No NSP: A left-context-only model which for MRPC which only has 3,600 labeled trainis trained using a standard Left-to-Right (LTR) ing examples, and is substantially different from LM, rather than an MLM. The left-only constraint the pre-training tasks. It is also perhaps surpriswas also applied at ne-tuning, because removing ing that we are able to achieve such signicant it introduced a pre-train/ne-tune mismatch that improvements on top of models which are aldegraded downstream performance. Additionally, ready quite large relative to the existing literature. this model was pre-trained without the NSP task. For example, the largest Transformer explored in This is directly comparable to OpenAI GPT, but Vaswani et al. (2017) is (L=6, H=1024, A=16) using our larger training dataset, our input reprewith 100M parameters for the encoder, and the sentation, and our ne-tuning scheme. largest Transformer we have found in the literature We rst examine the impact brought by the NSP is (L=64, H=512, A=2) with 235M parameters task. In Table 5, we show that removing NSP (Al-Rfou et al., 2018). By contrast, BERTBASE hurts performance signicantly on QNLI, MNLI, contains 110M parameters and BERTLARGE conand SQuAD 1.1. Next, we evaluate the impact tains 340M parameters. of training bidirectional representations by comIt has long been known that increasing the paring No NSP to LTR & No NSP. The LTR model size will lead to continual improvements model performs worse than the MLM model on all on large-scale tasks such as machine translation tasks, with large drops on MRPC and SQuAD. and language modeling, which is demonstrated For SQuAD it is intuitively clear that a LTR by the LM perplexity of held-out training data model will perform poorly at token predictions, shown in Table 6. However, we believe that since the token-level hidden states have no rightthis is the rst work to demonstrate convincside context. In order to make a good faith atingly that scaling to extreme model sizes also tempt at strengthening the LTR system, we added leads to large improvements on very small scale a randomly initialized BiLSTM on top. This does tasks, provided that the model has been suf-", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "48f5221c-1ef3-4a07-b3ed-e440be9fae59": {"__data__": {"id_": "48f5221c-1ef3-4a07-b3ed-e440be9fae59", "embedding": null, "metadata": {"page_number": 8, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "mixed results on the downstream task impact of System Dev F1 Test F1 increasing the pre-trained bi-LM size from two ELMo (Peters et al., 2018a) 95.7 92.2 to four layers and Melamud et al. (2016) menCVT (Clark et al., 2018) - 92.6 CSE (Akbik et al., 2018) - 93.1 tioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing Fine-tuning approach BERTLARGE 96.6 92.8 further to 1,000 did not bring further improveBERTBASE 96.4 92.4 ments. Both of these prior works used a featureFeature-based approach (BERTBASE) based approach we hypothesize that when the Embeddings 91.0 - model is ne-tuned directly on the downstream Second-to-Last Hidden 95.6 - Last Hidden 94.9 - tasks and uses only a very small number of ranWeighted Sum Last Four Hidden 95.9 - domly initialized additional parameters, the taskConcat Last Four Hidden 96.1 - specic models can benet from the larger, more Weighted Sum All 12 Layers 95.5 - expressive pre-trained representations even when Table 7: CoNLL-2003 Named Entity Recognition redownstream task data is very small. sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5.3 Feature-based Approach with BERT 5 random restarts using those hyperparameters. All of the BERT results presented so far have used the ne-tuning approach, where a simple classication layer is added to the pre-trained model, and layer in the output. We use the representation of all parameters are jointly ne-tuned on a downthe rst sub-token as the input to the token-level stream task. However, the feature-based approach, classier over the NER label set. where xed features are extracted from the preTo ablate the ne-tuning approach, we apply the trained model, has certain advantages. First, not feature-based approach by extracting the activaall tasks can be easily represented by a Transtions from one or more layers without ne-tuning former encoder architecture, and therefore require any parameters of BERT. These contextual ema task-specic model architecture to be added. beddings are used as input to a randomly initialSecond, there are major computational benets ized two-layer 768-dimensional BiLSTM before to pre-compute an expensive representation of the the classication layer. training data once and then run many experiments Results are presented in Table 7. BERTLARGEwith cheaper models on top of this representation. performs competitively with state-of-the-art methIn this section, we compare the two approaches ods. The best performing method concatenates the by applying BERT to the CoNLL-2003 Named token representations from the top four hidden layEntity Recognition (NER) task (Tjong Kim Sang ers of the pre-trained Transformer, which is only and De Meulder, 2003). In the input to BERT, we 0.3 F1 behind ne-tuning the entire model. This use a case-preserving WordPiece model, and we demonstrates that BERT is effective for both neinclude the maximal document context provided tuning and feature-based approaches. by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF 6 Conclusion Hyperparams Dev Set Accuracy #L #H #A LM (ppl) MNLI-m MRPC SST-2 Recent empirical improvements due to transfer learning with language models have demonstrated 3 768 12 5.84 77.9 79.8 88.4 6 768 3 5.24 80.6 82.2 90.7 that rich, unsupervised pre-training is an integral 6 768 12 4.68 81.9 84.8 91.3 part of many language understanding systems. In 12 768 12 3.99 84.4 86.7 92.9 12 1024 16 3.54 85.7 86.9 93.3 particular, these results enable even low-resource 24 1024 16 3.23 86.6 87.8 93.7 tasks to benet from deep unidirectional architectures. Our major contribution is further generalTable 6: Ablation over BERT model size. #L = the izing these ndings to deep bidirectional architecnumber of layers; #H = hidden size; #A = number of attures, allowing the same pre-trained model to suctention heads. LM (ppl) is the masked LM perplexity cessfully tackle a broad set of NLP tasks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "00ea391a-a03f-43b5-8990-dbaf3dda8965": {"__data__": {"id_": "00ea391a-a03f-43b5-8990-dbaf3dda8965", "embedding": null, "metadata": {"page_number": 9, "file_name": "app/database/sample/01-bert.pdf", "title": "BERT", "author": "Unknown"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "References Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. 2018. Semi-supervised seAlan Akbik, Duncan Blythe, and Roland Vollgraf. quence modeling with cross-view training. In Pro2018. Contextual string embeddings for sequence ceedings of the 2018 Conference on Empirical Methlabeling. In Proceedings of the 27th International ods in Natural Language Processing, pages 1914 Conference on Computational Linguistics, pages 1925. 16381649. Ronan Collobert and Jason Weston. 2008. A unied Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy architecture for natural language processing: Deep Guo, and Llion Jones. 2018. Character-level lanneural networks with multitask learning. In Proguage modeling with deeper self-attention. arXiv ceedings of the 25th international conference on preprint arXiv:1808.04444. Machine learning, pages 160167. ACM. Rie Kubota Ando and Tong Zhang. 2005. A framework Alexis Conneau, Douwe Kiela, Holger Schwenk, Loc for learning predictive structures from multiple tasks Barrault, and Antoine Bordes. 2017. Supervised and unlabeled data. Journal of Machine Learning learning of universal sentence representations from Research, 6(Nov):18171853. natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670680, CopenLuisa Bentivogli, Bernardo Magnini, Ido Dagan, hagen, Denmark. Association for Computational Hoa Trang Dang, and Danilo Giampiccolo. 2009. Linguistics. The fth PASCAL recognizing textual entailment challenge. In TAC. NIST. Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informaJohn Blitzer, Ryan McDonald, and Fernando Pereira. tion processing systems, pages 30793087. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conferJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Feience on empirical methods in natural language proFei. 2009. ImageNet: A Large-Scale Hierarchical cessing, pages 120128. Association for ComputaImage Database. In CVPR09. tional Linguistics. William B Dolan and Chris Brockett. 2005. AutomatiSamuel R. Bowman, Gabor Angeli, Christopher Potts, cally constructing a corpus of sentential paraphrases. and Christopher D. Manning. 2015. A large annoIn Proceedings of the Third International Workshop tated corpus for learning natural language inference. on Paraphrasing (IWP2005). In EMNLP. Association for Computational Linguistics. William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via lling in Peter F Brown, Peter V Desouza, Robert L Mercer, the . arXiv preprint arXiv:1801.07736. Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Dan Hendrycks and Kevin Gimpel. 2016. Bridging Computational linguistics, 18(4):467479. nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415. Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Gazpio, and Lucia Specia. 2017. Semeval-2017 Learning distributed representations of sentences task 1: Semantic textual similarity multilingual and from unlabelled data. In Proceedings of the 2016 crosslingual focused evaluation. In Proceedings Conference of the North American Chapter of the of the 11th International Workshop on Semantic Association for Computational Linguistics: Human Evaluation (SemEval-2017), pages 114, VancouLanguage Technologies. Association for Computaver, Canada. Association for Computational Lintional Linguistics. guistics. Jeremy Howard and Sebastian Ruder. 2018. Universal Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, language model ne-tuning for text classication. In Thorsten Brants, Phillipp Koehn, and Tony RobinACL. Association for Computational Linguistics. son. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, preprint arXiv:1312.3005. Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehenZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. sion. In IJCAI. Quora question pairs. Yacine Jernite, Samuel R. Bowman, and David SonChristopher Clark and Matt Gardner. 2018. Simple tag. 2017. Discourse-based objectives for fast unand effective multi-paragraph reading comprehensupervised sentence representation learning. CoRR,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "baaee4f1-fa42-4f89-91be-2a5f740e48ae": {"__data__": {"id_": "baaee4f1-fa42-4f89-91be-2a5f740e48ae", "embedding": null, "metadata": {"page_number": 0, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners Alec Radford * 1 Jeffrey Wu * 1 Rewon Child 1 David Luan 1 Dario Amodei ** 1 Ilya Sutskever ** 1 Abstract competent generalists. We would like to move towards more general systems which can perform many tasks eventually Natural language processing tasks, such as queswithout the need to manually create and label a training tion answering, machine translation, reading comdataset for each one. prehension, and summarization, are typically approached with supervised learning on taskThe dominant approach to creating ML systems is to colspecic datasets. We demonstrate that language lect a dataset of training examples demonstrating correct models begin to learn these tasks without any exbehavior for a desired task, train a system to imitate these plicit supervision when trained on a new dataset behaviors, and then test its performance on independent of millions of webpages called WebText. When and identically distributed (IID) held-out examples. This conditioned on a document plus questions, the anhas served well to make progress on narrow experts. But swers generated by the language model reach 55 the often erratic behavior of captioning models (Lake et al., F1 on the CoQA dataset - matching or exceeding 2017), reading comprehension systems (Jia & Liang, 2017), the performance of 3 out of 4 baseline systems and image classiers (Alcorn et al., 2018) on the diversity without using the 127,000+ training examples. and variety of possible inputs highlights some of the shortThe capacity of the language model is essential comings of this approach. to the success of zero-shot task transfer and inOur suspicion is that the prevalence of single task training creasing it improves performance in a log-linear on single domain datasets is a major contributor to the lack fashion across tasks. Our largest model, GPT-2, of generalization observed in current systems. Progress is a 1.5B parameter Transformer that achieves towards robust systems with current architectures is likely state of the art results on 7 out of 8 tested lanto require training and measuring performance on a wide guage modeling datasets in a zero-shot setting range of domains and tasks. Recently, several benchmarks but still underts WebText. Samples from the have been proposed such as GLUE (Wang et al., 2018) and model reect these improvements and contain codecaNLP (McCann et al., 2018) to begin studying this. herent paragraphs of text. These ndings suggest a promising path towards building language proMultitask learning (Caruana, 1997) is a promising framecessing systems which learn to perform tasks from work for improving general performance. However, multheir naturally occurring demonstrations. titask training in NLP is still nascent. Recent work reports modest performance improvements (Yogatama et al., 2019) and the two most ambitious efforts to date have trained on a total of 10 and 17 (dataset, objective) 1. Introduction pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning perspective, each (dataset, Machine learning systems now excel (in expectation) at objective) pair is a single training example sampled tasks they are trained for by using a combination of large from the distribution of datasets and objectives. Current datasets, high-capacity models, and supervised learning ML systems need hundreds to thousands of examples to (Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei induce functions which generalize well. This suggests that et al., 2016). Yet these systems are brittle and sensitive to multitask training many need just as many effective training slight changes in the data distribution (Recht et al., 2018) pairs to realize its promise with current approaches. It will and task specication (Kirkpatrick et al., 2017). Current sysbe very difcult to continue to scale the creation of datasets tems are better characterized as narrow experts rather than and the design of objectives to the degree that may be re- *, quired to brute force our way there with current techniques. **Equal contribution 1OpenAI, San Francisco, CaliforThis motivates exploring additional setups for performingnia, United States. Correspondence to: Alec Radford <alec@openai.com>. multitask learning.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "f1b2cbe9-1ac0-41d5-8793-d43a05ae0444": {"__data__": {"id_": "f1b2cbe9-1ac0-41d5-8793-d43a05ae0444", "embedding": null, "metadata": {"page_number": 1, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners Figure 1. Zero-shot task performance of WebText LMs as a function of model size on many NLP tasks. Reading Comprehension results are on CoQA (Reddy et al., 2018), translation on WMT-14 Fr-En (Artetxe et al., 2017), summarization on CNN and Daily Mail (See et al., 2017), and Question Answering on Natural Questions (Kwiatkowski et al., 2019). Section 3 contains detailed descriptions of each result. utilize a combination of pre-training and supervised nesymbols as the product of conditional probabilities (Jelinek tuning. This approach has a long history with a trend to- & Mercer, 1980) (Bengio et al., 2003): wards more exible forms of transfer. First, word vectors were learned and used as inputs to task-specic architecn tures (Mikolov et al., 2013) (Collobert et al., 2011), then p(x) = Y p(sn|s1, ..., sn1) (1) the contextual representations of recurrent networks were i=1 transferred (Dai & Le, 2015) (Peters et al., 2018), and recent work suggests that task-specic architectures are no This approach allows for tractable sampling from and eslonger necessary and transferring many self-attention blocks timation of p(x) as well as any conditionals of the form is sufcient (Radford et al., 2018) (Devlin et al., 2018). p(snk, ..., sn|s1, ..., snk1). In recent years, there have These methods still require supervised training in order been signicant improvements in the expressiveness of modto perform a task. When only minimal or no supervised els that can compute these conditional probabilities, such as data is available, another line of work has demonstrated self-attention architectures like the Transformer (Vaswani the promise of language models to perform specic tasks, et al., 2017). such as commonsense reasoning (Schwartz et al., 2017) and Learning to perform a single task can be expressed in a sentiment analysis (Radford et al., 2017). probabilistic framework as estimating a conditional distriIn this paper, we connect these two lines of work and conbution p(output|input). Since a general system should be tinue the trend of more general methods of transfer. We able to perform many different tasks, even for the same demonstrate language models can perform down-stream input, it should condition not only on the input but also tasks in a zero-shot setting without any parameter or archion the task to be performed. That is, it should model tecture modication. We demonstrate this approach shows p(output|input, task). This has been variously formalized potential by highlighting the ability of language models to in multitask and meta-learning settings. Task conditioning perform a wide range of tasks in a zero-shot setting. We is often implemented at an architectural level, such as the achieve promising, competitive, and state of the art results task specic encoders and decoders in (Kaiser et al., 2017) depending on the task. or at an algorithmic level such as the inner and outer loop optimization framework of MAML (Finn et al., 2017). But as exemplied in McCann et al. (2018), language provides 2. Approach a exible way to specify tasks, inputs, and outputs all as a At the core of our approach is language modeling. Lansequence of symbols. For example, a translation training guage modeling is usually framed as unsupervised distriexample can be written as the sequence (translate to bution estimation from a set of examples (x1, x2, ..., xn) french, english text, french text). Likeeach composed of variable length sequences of symbols wise, a reading comprehension training example can (s1, s2, ..., sn). Since language has a natural sequential orbe written as (answer the question, document, dering, it is common to factorize the joint probabilities over question, answer). McCann et al. (2018) demon-", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "b8a59434-fca7-4b7f-a4aa-38ac34fc1d8d": {"__data__": {"id_": "b8a59434-fca7-4b7f-a4aa-38ac34fc1d8d", "embedding": null, "metadata": {"page_number": 2, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners to infer and perform many different tasks on examples with Im not the cleverest man in the world, but like they say in this type of format. French: Je ne suis pas un imbecile [Im not a fool]. Language modeling is also able to, in principle, learn the In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate in the riding of Joliette, wrote in French: Mentez mentez,tasks of McCann et al. (2018) without the need for explicit il en restera toujours quelque chose, which translates as, supervision of which symbols are the outputs to be preLie lie and something will always remain. dicted. Since the supervised objective is the the same as the unsupervised objective but only evaluated on a subset of the I hate the word perfume, Burr says. Its somewhat better in French: parfum. sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective. In If listened carefully at 29:55, a conversation can be heard this slightly toy setting, the concerns with density estimation between two guys in French: -Comment on fait pour aller de lautre cote? -Quel autre cote?, which means - Howas a principled training objective discussed in (Sutskever do you get to the other side? - What side?. et al., 2015) are side stepped. The problem instead becomes whether we are able to, in practice, optimize the unsuperIf this sounds like a bit of a stretch, consider this quesvised objective to convergence. Preliminary experiments tion in French: As-tu aller au cinema?, or Did you go to the movies?, which literally translates as Have-you to go to conrmed that sufciently large language models are able to movies/theater? perform multitask learning in this toy-ish setup but learning is much slower than in explicitly supervised approaches. Brevet Sans Garantie Du Gouvernement, translated to English: Patented without government warranty. While it is a large step from the well-posed setup described above to the messiness of language in the wild, Weston Table 1. Examples of naturally occurring demonstrations of En-(2016) argues, in the context of dialog, for the need to glish to French and French to English translation found throughout develop systems capable of learning from natural language the WebText training set. directly and demonstrated a proof of concept learning a QA task without a reward signal by using forward prediction of a teachers outputs. While dialog is an attractive approach, we worry it is overly restrictive. The internet contains a vast Common Crawl. Trinh & Le (2018)s best results were amount of information that is passively available without achieved using a small subsample of Common Crawl which the need for interactive communication. Our speculation is included only documents most similar to their target dataset, that a language model with sufcient capacity will begin the Winograd Schema Challenge. While this is a pragmatic to learn to infer and perform the tasks demonstrated in approach to improve performance on a specic task, we natural language sequences in order to better predict them, want to avoid making assumptions about the tasks to be regardless of their method of procurement. If a language performed ahead of time. model is able to do this it will be, in effect, performing Instead, we created a new web scrape which emphasizes unsupervised multitask learning. We test whether this is the document quality. To do this we only scraped web pages case by analyzing the performance of language models in a which have been curated/ltered by humans. Manually zero-shot setting on a wide variety of tasks. ltering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from 2.1. Training Dataset Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator forMost prior work trained language models on a single dowhether other users found the link interesting, educational,main of text, such as news articles (Jozefowicz et al., 2016), or just funny.Wikipedia (Merity et al., 2016), or ction books (Kiros et al., 2015). Our approach motivates building as large and The resulting dataset, WebText, contains the text subset diverse a dataset as possible in order to collect natural lanof these 45 million links. To extract the text from HTML guage demonstrations of tasks in as varied of domains and responses we use a combination of the Dragnet (Peters & contexts as possible. Lecocq, 2013) and Newspaper1 content extractors. All results presented in this paper use a preliminary version ofA promising source of diverse and nearly unlimited text is WebText which does not include links created after Decweb scrapes such as Common Crawl. While these archives 2017 and which after de-duplication and some heuristicare many orders of magnitude larger than current language based cleaning contains slightly over 8 million documentsmodeling datasets, they have signicant data quality issues. for a total of 40 GB of text. We removed all WikipediaTrinh & Le (2018) used Common Crawl in their work on documents from WebText since it is a common data sourcecommonsense reasoning but noted a large amount of docfor other datasets and could complicate analysis due to over-uments whose content are mostly unintelligible. We ob-", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "2081816a-3769-4ae2-89e5-7beccd142ab9": {"__data__": {"id_": "2081816a-3769-4ae2-89e5-7beccd142ab9", "embedding": null, "metadata": {"page_number": 3, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners lapping training data with test evaluation tasks. Parameters Layers dmodel 2.2. Input Representation 117M 12 768 345M 24 1024 A general language model (LM) should be able to compute 762M 36 1280 the probability of (and also generate) any string. Current 1542M 48 1600 large scale LMs include pre-processing steps such as lowercasing, tokenization, and out-of-vocabulary tokens which Table 2. Architecture hyperparameters for the 4 model sizes. restrict the space of model-able strings. While processing Unicode strings as a sequence of UTF-8 bytes elegantly fullls this requirement as exemplied in work such as Gillick few modications. Layer normalization (Ba et al., 2016) et al. (2015), current byte-level LMs are not competitive was moved to the input of each sub-block, similar to a with word-level LMs on large scale datasets such as the pre-activation residual network (He et al., 2016) and an One Billion Word Benchmark (Al-Rfou et al., 2018). We additional layer normalization was added after the nal selfobserved a similar performance gap in our own attempts to attention block. A modied initialization which accounts train standard byte-level LMs on WebText. for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initial-Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a ization by a factor of 1/ N where N is the number of practical middle ground between character and word level residual layers. The vocabulary is expanded to 50,257. We language modeling which effectively interpolates between also increase the context size from 512 to 1024 tokens and word level inputs for frequent symbol sequences and chara larger batchsize of 512 is used. acter level inputs for infrequent symbol sequences. Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These imple3. Experiments mentations would require including the full space of UniWe trained and benchmarked four LMs with approximately code symbols in order to model all Unicode strings. This log-uniformly spaced sizes. The architectures are summawould result in a base vocabulary of over 130,000 before rized in Table 2. The smallest model is equivalent to the any multi-symbol tokens are added. This is prohibitively original GPT, and the second smallest equivalent to the large compared to the 32,000 to 64,000 token vocabularies largest model from BERT (Devlin et al., 2018). Our largest often used with BPE. In contrast, a byte-level version of model, which we call GPT-2, has over an order of magniBPE only requires a base vocabulary of size 256. However, tude more parameters than GPT. The learning rate of each directly applying BPE to the byte sequence results in submodel was manually tuned for the best perplexity on a 5% optimal merges due to BPE using a greedy frequency based held-out sample of WebText. All models still undert Webheuristic for building the token vocabulary. We observed Text and held-out perplexity has as of yet improved given BPE including many versions of common words like dog more training time. since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited 3.1. Language Modelingvocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any As an initial step towards zero-shot task transfer, we are byte sequence. We add an exception for spaces which siginterested in understanding how WebText LMs perform nicantly improves the compression efciency while adding at zero-shot domain transfer on the primary task they are only minimal fragmentation of words across multiple vocab trained for language modeling. Since our model operates tokens. on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language modelThis input representation allows us to combine the empirical benchmark. Results on language modeling datasets arebenets of word-level LMs with the generality of byte-level commonly reported in a quantity which is a scaled or ex-approaches. Since our approach can assign a probability to ponentiated version of the average negative log probabilityany Unicode string, this allows us to evaluate our LMs on per canonical prediction unit - usually a character, a byte, orany dataset regardless of pre-processing, tokenization, or a word. We evaluate the same quantity by computing thevocab size. log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these2.3. Model datasets, WebText LMs would be tested signicantly outWe use a Transformer (Vaswani et al., 2017) based archiof-distribution, having to predict aggressively standardized tecture for our LMs. The model largely follows the details text, tokenization artifacts such as disconnected punctuation", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "0826c996-847b-46e4-963e-abf51ae2df08": {"__data__": {"id_": "0826c996-847b-46e4-963e-abf51ae2df08", "embedding": null, "metadata": {"page_number": 4, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners LAMBADA LAMBADA CBT-CN CBT-NE WikiText2 PTB enwik8 text8 WikiText103 1BW (PPL) (ACC) (ACC) (ACC) (PPL) (PPL) (BPB) (BPC) (PPL) (PPL) SOTA 99.8 59.23 85.7 82.3 39.14 46.54 0.99 1.08 18.3 21.8 117M 35.13 45.99 87.65 83.4 29.41 65.85 1.16 1.17 37.50 75.20 345M 15.60 55.48 92.35 87.1 22.76 47.33 1.01 1.06 26.37 55.72 762M 10.87 60.12 93.45 88.0 19.93 40.31 0.97 1.02 22.05 44.575 1542M 8.63 63.24 93.30 89.05 18.34 35.76 0.93 0.98 17.48 42.16 Table 3. Zero-shot results on many datasets. No training or ne-tuning was performed for any of these results. PTB and WikiText-2 results are from (Gong et al., 2018). CBT results are from (Bajgar et al., 2016). LAMBADA accuracy result is from (Hoang et al., 2018) and LAMBADA perplexity result is from (Grave et al., 2016). Other results are from (Dai et al., 2019). <UNK> which is extremely rare in WebText - occurring The Childrens Book Test (CBT) (Hill et al., 2015) was only 26 times in 40 billion bytes. We report our main recreated to examine the performance of LMs on different catsults in Table 3 using invertible de-tokenizers which remove egories of words: named entities, nouns, verbs, and preposias many of these tokenization / pre-processing artifacts as tions. Rather than reporting perplexity as an evaluation metpossible. Since these de-tokenizers are invertible, we can ric, CBT reports accuracy on an automatically constructed still calculate the log probability of a dataset and they can cloze test where the task is to predict which of 10 possible be thought of as a simple form of domain adaptation. We choices for an omitted word is correct. Following the LM observe gains of 2.5 to 5 perplexity for GPT-2 with these approach introduced in the original paper, we compute the de-tokenizers. probability of each choice and the rest of the sentence conditioned on this choice according to the LM, and predict WebText LMs transfer well across domains and datasets, the one with the highest probability. As seen in Figure 2 improving the state of the art on 7 out of the 8 datasets in a performance steadily improves as model size is increased zero-shot setting. Large improvements are noticed on small and closes the majority of the gap to human performance datasets such as Penn Treebank and WikiText-2 which have on this test. Data overlap analysis showed one of the CBT only 1 to 2 million training tokens. Large improvements test set books, The Jungle Book by Rudyard Kipling, is in are also noticed on datasets created to measure long-term WebText, so we report results on the validation set which dependencies like LAMBADA (Paperno et al., 2016) and has no signicant overlap. GPT-2 achieves new state of the the Childrens Book Test (Hill et al., 2015). Our model is art results of 93.3% on common nouns and 89.1% on named still signicantly worse than prior work on the One Billion entities. A de-tokenizer was applied to remove PTB style Word Benchmark (Chelba et al., 2013). This is likely due tokenization artifacts from CBT. to a combination of it being both the largest dataset and having some of the most destructive pre-processing - 1BWs 3.3. LAMBADAsentence level shufing removes all long-range structure. The LAMBADA dataset (Paperno et al., 2016) tests the 3.2. Childrens Book Test ability of systems to model long-range dependencies in text. The task is to predict the nal word of sentences which require at least 50 tokens of context for a human to successfully predict. GPT-2 improves the state of the art from 99.8 (Grave et al., 2016) to 8.6 perplexity and increases the accuracy of LMs on this test from 19% (Dehghani et al., 2018) to 52.66%. Investigating GPT-2s errors showed most predictions are valid continuations of the sentence, but are not valid nal words. This suggests that the LM is not using the additional useful constraint that the word must be the nal of the sentence. Adding a stop-word lter as an approximation to this further increases accuracy to 63.24%, improving the overall state of the art on this task by 4%. The previous state of the art (Hoang et al., 2018) used a differentFigure 2. Performance on the Childrens Book Test as a function of restricted prediction setting where the outputs of the modelmodel capacity. Human performance are from Bajgar et al. (2016), instead of the much lower estimates from the original paper. were constrained to only words that appeared in the context.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "602481c4-6b4b-4649-9c8d-dbba6167cf29": {"__data__": {"id_": "602481c4-6b4b-4649-9c8d-dbba6167cf29", "embedding": null, "metadata": {"page_number": 5, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners since 19% of answers are not in context. We use a version of the dataset without preprocessing. R-1 R-2 R-L R-AVG Bottom-Up Sum 41.22 18.68 38.34 32.75 3.4. Winograd Schema Challenge Lede-3 40.38 17.66 36.62 31.55 Seq2Seq + Attn 31.33 11.81 28.83 23.99 GPT-2 TL;DR: 29.34 8.27 26.58 21.40 Random-3 28.78 8.63 25.52 20.98 GPT-2 no hint 21.58 4.03 19.47 15.03 Table 4. Summarization performance as measured by ROUGE F1 metrics on the CNN and Daily Mail dataset. Bottom-Up Sum is the SOTA model from (Gehrmann et al., 2018) 2018), is nearing the 89 F1 performance of humans. While GPT-2s performance is exciting for a system without any supervised training, some inspection of its answers and errors suggests GPT-2 often uses simple retrieval based heuristics such as answer with a name from the document in response to a who question. Figure 3. Performance on the Winograd Schema Challenge as a 3.6. Summarization function of model capacity. We test GPT-2s ability to perform summarization on the CNN and Daily Mail dataset (Nallapati et al., 2016). To induce summarization behavior we add the text TL;DR: after The Winograd Schema challenge (Levesque et al., 2012) the article and generate 100 tokens with Top-k random samwas constructed to measure the capability of a system to pling (Fan et al., 2018) with k = 2 which reduces repetition perform commonsense reasoning by measuring its ability and encourages more abstractive summaries than greedy deto resolve ambiguities in text. Recently Trinh & Le (2018) coding. We use the rst 3 generated sentences in these 100 demonstrated signicant progress on this challenge using tokens as the summary. While qualitatively the generations LMs, by predicting the resolution of the ambiguity with resemble summaries, as shown in Table 14, they often focus higher probability. We follow their problem formulation and on recent content from the article or confuse specic details visualize the performance of our models with both full and such as how many cars were involved in a crash or whether partial scoring techniques in Figure 3. GPT-2 improves state a logo was on a hat or shirt. On the commonly reported of the art accuracy by 7%, achieving 70.70%. The dataset ROUGE 1,2,L metrics the generated summaries only begin is quite small with only 273 examples so we recommend to approach the performance of classic neural baselines and reading Trichelair et al. (2018) to help contextualize this just barely outperforms selecting 3 random sentences from result. the article. GPT-2s performance drops by 6.4 points on the aggregate metric when the task hint is removed which 3.5. Reading Comprehension demonstrates the ability to invoke task specic behavior in The Conversation Question Answering dataset (CoQA) a language model with natural language. Reddy et al. (2018) consists of documents from 7 different domains paired with natural language dialogues between a 3.7. Translation question asker and a question answerer about the document. We test whether GPT-2 has begun to learn how to translate CoQA tests reading comprehension capabilities and also from one language to another. In order to help it infer that the ability of models to answer questions that depend on this is the desired task, we condition the language model conversation history (such as Why?). on a context of example pairs of the format english Greedy decoding from GPT-2 when conditioned on a docsentence = french sentence and then after a - ument, the history of the associated conversation, and a nal prompt of english sentence = we sample from nal token A: achieves 55 F1 on the development set. This the model with greedy decoding and use the rst generated matches or exceeds the performance of 3 out of 4 basesentence as the translation. On the WMT-14 English-French line systems without using the 127,000+ manually collected test set, GPT-2 gets 5 BLEU, which is slightly worse than question answer pairs those baselines were trained on. The a word-by-word substitution with a bilingual lexicon in-", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "486adf83-a4c9-4d5c-8f8b-f1c483ac5894": {"__data__": {"id_": "486adf83-a4c9-4d5c-8f8b-f1c483ac5894", "embedding": null, "metadata": {"page_number": 6, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners Question Generated Answer Correct Probability Who wrote the book the origin of species? Charles Darwin 83.4% Who is the founder of the ubuntu project? Mark Shuttleworth 82.0% Who is the quarterback for the green bay packers? Aaron Rodgers 81.1% Panda is a national animal of which country? China 76.8% Who came up with the theory of relativity? Albert Einstein 76.4% When was the rst star wars lm released? 1977 71.4% What is the most common blood type in sweden? A 70.6% Who is regarded as the founder of psychoanalysis? Sigmund Freud 69.3% Who took the rst steps on the moon in 1969? Neil Armstrong 66.8% Who is the largest supermarket chain in the uk? Tesco 65.3% What is the meaning of shalom in english? peace 64.0% Who was the author of the art of war? Sun Tzu 59.6% Largest state in the us by land mass? California 59.2% Green algae is an example of which type of reproduction? parthenogenesis 56.5% Vikram samvat calender is ofcial in which country? India 55.6% Who is mostly responsible for writing the declaration of independence? Thomas Jefferson 53.3% What us state forms the western boundary of montana? Montana 52.3% Who plays ser davos in game of thrones? Peter Dinklage 52.1% Who appoints the chair of the federal reserve system? Janet Yellen 51.5% State the process that divides one nucleus into two genetically identical nuclei? mitosis 50.7% Who won the most mvp awards in the nba? Michael Jordan 50.2% What river is associated with the city of rome? the Tiber 48.6% Who is the rst president to be impeached? Andrew Johnson 48.3% Who is the head of the department of homeland security 2017? John Kelly 47.0% What is the name given to the common currency to the european union? Euro 46.8% What was the emperor name in star wars? Palpatine 46.5% Do you have to have a gun permit to shoot at a range? No 46.4% Who proposed evolution in 1859 as the basis of biological development? Charles Darwin 45.7% Nuclear power plant that blew up in russia? Chernobyl 45.7% Who played john connor in the original terminator? Arnold Schwarzenegger 45.2% Table 5. The 30 most condent answers generated by GPT-2 on the development set of Natural Questions sorted by their probability according to GPT-2. None of these questions appear in WebText according to the procedure described in Section 4. (Conneau et al., 2017b). On the WMT-14 French-English 2019) is a promising resource to test this more quantitatest set, GPT-2 is able to leverage its very strong English tively. Similar to translation, the context of the language language model to perform signicantly better, achieving model is seeded with example question answer pairs which 11.5 BLEU. This outperforms several unsupervised machine helps the model infer the short answer style of the dataset. translation baselines from (Artetxe et al., 2017) and (Lample GPT-2 answers 4.1% of questions correctly when evaluet al., 2017) but is still much worse than the 33.5 BLEU of ated by the exact match metric commonly used on reading the current best unsupervised machine translation approach comprehension datasets like SQUAD.3 As a comparison (Artetxe et al., 2019). Performance on this task was surpoint, the smallest model does not exceed the 1.0% accuprising to us, since we deliberately removed non-English racy of an incredibly simple baseline which returns the most webpages from WebText as a ltering step. In order to concommon answer for each question type (who, what, where, rm this, we ran a byte-level language detector2 on WebText etc...). GPT-2 answers 5.3 times more questions correctly, which detected only 10MB of data in the French language suggesting that model capacity has been a major factor in which is approximately 500x smaller than the monolingual the poor performance of neural systems on this kind of task French corpus common in prior unsupervised machine transas of yet. The probability GPT-2 assigns to its generated lation research. answers is well calibrated and GPT-2 has an accuracy of 63.1% on the 1% of questions it is most condent in. The 3.8. Question Answering 30 most condent answers generated by GPT-2 on development set questions are shown in Table 5. The performance A potential way to test what information is contained within of GPT-2 is still much, much, worse than the 30 to 50% a language model is to evaluate how often it generates the range of open domain question answering systems which correct answer to factoid-style questions. Previous showcashybridize information retrieval with extractive document ing of this behavior in neural systems where all information question answering (Alberti et al., 2019). is stored in parameters such as A Neural Conversational Model (Vinyals & Le, 2015) reported qualitative results due 3Alec, who previously thought of himself as good at random trivia, answered 17 of 100 randomly sampled examples correctlyto the lack of high-quality evaluation datasets. The recently when tested in the same setting as GPT-2. He actually only got 14 right but he introduced Natural Questions dataset (Kwiatkowski et al., should have gotten those other 3", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "ed694ae6-7d75-461e-94e6-5ea880b4d67f": {"__data__": {"id_": "ed694ae6-7d75-461e-94e6-5ea880b4d67f", "embedding": null, "metadata": {"page_number": 7, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners PTB WikiText-2 enwik8 text8 Wikitext-103 1BW Dataset train 2.67% 0.66% 7.50% 2.34% 9.09% 13.19% WebText train 0.88% 1.63% 6.31% 3.94% 2.42% 3.75% Table 6. Percentage of test set 8 grams overlapping with training sets. 4. Generalization vs Memorization gave away the answer. Recent work in computer vision has shown that common imFor CoQA, about 15% of documents in the news domain age datasets contain a non-trivial amount of near-duplicate are already in WebText and the model performs about 3 images. For instance CIFAR-10 has 3.3% overlap between F1 better on these. CoQAs development set metric reports train and test images (Barz & Denzler, 2019). This results in the average performance over 5 different domains and we an over-reporting of the generalization performance of mameasure a gain of about 0.5-1.0 F1 due to overlap across the chine learning systems. As the size of datasets increases this various domains. However, no actual training questions or issue becomes increasingly likely which suggests a similar answers are in WebText since CoQA was released after the phenomena could be happening with WebText. Therefore it cutoff date for links in WebText. is important to analyze how much test data also shows up in On LAMBADA, the average overlap is 1.2%. GPT-2 perthe training data. forms about 2 perplexity better on examples with greater To study this we created Bloom lters containing 8-grams than 15% overlap. Recalculating metrics when excluding of WebText training set tokens. To improve recall, strings all examples with any overlap shifts results from 8.6 to 8.7 were normalized to contain only lower-cased alphanumeric perplexity and reduces accuracy from 63.2% to 62.9%. This words with a single space as a delimiter. The Bloom lters very small change in overall results is likely due to only 1 were constructed such that the false positive rate is upper in 200 examples having signicant overlap. bounded by 1081 . We further veried the low false positive Overall, our analysis suggests that data overlap between rate by generating 1M strings, of which zero were found by WebText training data and specic evaluation datasets prothe lter. vides a small but consistent benet to reported results. HowThese Bloom lters let us calculate, given a dataset, the ever, for most datasets we do not notice signicantly larger percentage of 8-grams from that dataset that are also found overlaps than those already existing between standard trainin the WebText training set. Table 6 shows this overlap analing and test sets, as Table 6 highlights. ysis for the test sets of common LM benchmarks. Common Understanding and quantifying how highly similar text imLM datasets test sets have between 1-6% overlap with Webpacts performance is an important research question. Better Text train, with an average of overlap of 3.2%. Somewhat de-duplication techniques such as scalable fuzzy matching surprisingly, many datasets have larger overlaps with their could also help better answer these questions. For now, we own training splits, with an average of 5.9% overlap. recommend the use of n-gram overlap based de-duplication Our approach optimizes for recall, and while manual inspecas an important verication step and sanity check during the tion of the overlaps shows many common phrases, there are creation of training and test splits for new NLP datasets. many longer matches that are due to duplicated data. This is Another potential way of determining whether the perfornot unique to WebText. For instance, we discovered that the mance of WebText LMs is attributable to memorization is test set of WikiText-103 has an article which is also in the inspecting their performance on their own held-out set. As training dataset. Since there are only 60 articles in the test shown in Figure 4, performance on both the training and set there is at least an overlap of 1.6%.4 Potentially more test sets of WebText are similar and improve together as worryingly, 1BW has an overlap of nearly 13.2% with its model size is increased. This suggests even GPT-2 is still own training set according to our procedure. undertting on WebText in many ways. For the Winograd Schema Challenge, we found only 10 GPT-2 is also able to write news articles about the discovery schemata which had any 8-gram overlaps with the WebText of talking unicorns. An example is provided in Table 13. training set. Of these, 2 were spurious matches. Of the remaining 8, only 1 schema appeared in any contexts that 5. Related Work 4A signicant portion of additional overlap is due to editors reusing some paragraphs across multiple articles with a shared A signicant portion of this work measured the performance", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "d3fa1e96-f23f-4a61-bdb8-2e110988b99d": {"__data__": {"id_": "d3fa1e96-f23f-4a61-bdb8-2e110988b99d", "embedding": null, "metadata": {"page_number": 8, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners improved the RNN based ne-tuning approaches of (Dai & Le, 2015). (Conneau et al., 2017a) studied the transfer performance of representations learned by natural language inference models and (Subramanian et al., 2018) explored large-scale multitask training. (Ramachandran et al., 2016) demonstrated that seq2seq models benet from being initialized with pre-trained language models as encoders and decoders. More recent work has shown that LM pre-training is helpful when ne-tuned for difcult generation tasks like chit-chat dialog and dialog based question answering systems as well (Wolf et al., 2019) (Dinan et al., 2018). 6. Discussion Much research has been dedicated to learning (Hill et al., 2016), understanding (Levy & Goldberg, 2014), and critically evaluating (Wieting & Kiela, 2019) the representations of both supervised and unsupervised pre-training methods.Figure 4. The performance of LMs trained on WebText as a function of model size. Our results suggest that unsupervised task learning is an additional promising area of research to explore. These ndings potentially help explain the widespread success of pre-training techniques for down-stream NLP tasks as we show that, in the limit, one of these pre-training techniquesis similar to the work of Jozefowicz et al. (2016) which begins to learn to perform tasks directly without the needscaled RNN based language models on the 1 Billion Word for supervised adaption or modication.Benchmark. Bajgar et al. (2016) also previously improved results on the Childrens Book Test by creating a much larger On reading comprehension the performance of GPT-2 is training dataset out of Project Gutenberg to supplement the competitive with supervised baselines in a zero-shot setting. standard training dataset. Hestness et al. (2017) conducted However, on other tasks such as summarization, while it a thorough analysis of how the performance of various deep is qualitatively performing the task, its performance is still learning models changes as a function of both model capaconly rudimentary according to quantitative metrics. While ity and dataset size. Our experiments, while much noisier suggestive as a research result, in terms of practical applicaacross tasks, suggest similar trends hold for sub-tasks of an tions, the zero-shot performance of GPT-2 is still far from objective and continue into the 1B+ parameter regime. use-able. Interesting learned functionality in generative models We have studied the zero-shot performance of WebText has been documented before such as the cells in an LMs on many canonical NLP tasks, but there are many addiRNN language model performing line-width tracking and tional tasks that could be evaluated. There are undoubtedly quote/comment detection Karpathy et al. (2015). More inmany practical tasks where the performance of GPT-2 is spirational to our work was the observation of Liu et al. still no better than random. Even on common tasks that we (2018) that a model trained to generate Wikipedia articles evaluated on, such as question answering and translation, also learned to translate names between languages. language models only begin to outperform trivial baselines when they have sufcient capacity.Previous work has explored alternative approaches to ltering and constructing a large text corpus of web pages, such While zero-shot performance establishes a baseline of the as the iWeb Corpus (Davies, 2018). potential performance of GPT-2 on many tasks, it is not clear where the ceiling is with netuning. On some tasks,There has been extensive work on pre-training methods GPT-2s fully abstractive output is a signicant departurefor language tasks. In addition to those mentioned in the from the extractive pointer network (Vinyals et al., 2015)introduction, GloVe (Pennington et al., 2014) scaled word based outputs which are currently state of the art on manyvector representation learning to all of Common Crawl. An question answering and reading comprehension datasets.inuential early work on deep representation learning for Given the prior success of ne-tuning GPT, we plan to in-text was Skip-thought Vectors (Kiros et al., 2015). McCann vestigate ne-tuning on benchmarks such as decaNLP andet al. (2017) explored the use of representations derived from", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "4fcf2312-cd07-4725-9e7c-5589bcf5a3b8": {"__data__": {"id_": "4fcf2312-cd07-4725-9e7c-5589bcf5a3b8", "embedding": null, "metadata": {"page_number": 9, "file_name": "app/database/sample/01-gpt-2.pdf", "title": "GPT 2", "author": "Balaji"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Language Models are Unsupervised Multitask Learners training data and capacity of GPT-2 is sufcient to overBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. come the inefciencies of uni-directional representations arXiv preprint arXiv:1607.06450, 2016. demonstrated by BERT (Devlin et al., 2018). Bajgar, O., Kadlec, R., and Kleindienst, J. Embracing data abundance: Booktest dataset for reading comprehension. arXiv 7. Conclusion preprint arXiv:1610.00956, 2016. Barz, B. and Denzler, J. Do we train on test data? purging cifar ofWhen a large language model is trained on a sufciently near-duplicates. arXiv preprint arXiv:1902.00423, 2019. large and diverse dataset it is able to perform well across many domains and datasets. GPT-2 zero-shots to state of Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A neural the art performance on 7 out of 8 tested language modelprobabilistic language model. Journal of machine learning research, 3(Feb):11371155, 2003.ing datasets. The diversity of tasks the model is able to perform in a zero-shot setting suggests that high-capacity Bowman, S. R., Pavlick, E., Grave, E., Van Durme, B., Wang, A., models trained to maximize the likelihood of a sufciently Hula, J., Xia, P., Pappagari, R., McCoy, R. T., Patel, R., et al. varied text corpus begin to learn how to perform a surprising Looking for elmos friends: Sentence-level pretraining beyond amount of tasks without the need for explicit supervision.5 language modeling. arXiv preprint arXiv:1812.10860, 2018. Caruana, R. Multitask learning. Machine learning, 28(1):4175, Acknowledgements 1997. Thanks to everyone who wrote the text, shared the links, Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. One billion word benchmark for measurand upvoted the content in WebText. Many millions of ing progress in statistical language modeling. arXiv preprint people were involved in creating the data that GPT-2 was arXiv:1312.3005, 2013. trained on. Also thanks to all the Googlers who helped us with training infrastructure, including Zak Stone, JS Riehl, Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) fromJonathan Hseu, Russell Power, Youlong Cheng, Noam scratch. Journal of Machine Learning Research, 12(Aug):2493 Shazeer, Solomon Boulos, Michael Baneld, Aman Gupta, 2537, 2011. Daniel Sohn, and many more. Finally thanks to the people who gave feedback on drafts of the paper: Jacob Steinhardt, Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. Supervised learning of universal sentence represen-Sam Bowman, Geoffrey Irving, and Madison May. tations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017a. References Conneau, A., Lample, G., Ranzato, M., Denoyer, L., and Jegou, Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L. H. Word translation without parallel data. arXiv preprint Character-level language modeling with deeper self-attention. arXiv:1710.04087, 2017b. arXiv preprint arXiv:1808.04444, 2018. Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. In Alberti, C., Lee, K., and Collins, M. A bert baseline for the natural Advances in neural information processing systems, pp. 3079 questions. arXiv preprint arXiv:1901.08634, 2019. 3087, 2015. Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le,Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-S., and Q. V., and Salakhutdinov, R. Transformer-xl: Attentive lanNguyen, A. Strike (with) a pose: Neural networks are easily guage models beyond a xed-length context. arXiv preprint fooled by strange poses of familiar objects. arXiv preprint arXiv:1901.02860, 2019. arXiv:1811.11553, 2018. Davies, M. The 14 billion word iweb corpus. Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenhttps://corpus.byu.edu/iWeb/, 2018. berg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen, G., et al. Deep speech 2: End-to-end speech recognition in Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, english and mandarin. In International Conference on Machine . Universal transformers. arXiv preprint arXiv:1807.03819, Learning, pp. 173182, 2016. 2018. Artetxe, M., Labaka, G., Agirre, E., and Cho, K. Unsupervised Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Preneural machine translation. arXiv preprint arXiv:1710.11041, training of deep bidirectional transformers for language under2017. standing. arXiv preprint arXiv:1810.04805, 2018. Artetxe, M., Labaka, G., and Agirre, E. An effective apDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, proach to unsupervised machine translation. arXiv preprint J. Wizard of wikipedia: Knowledge-powered conversational arXiv:1902.01313, 2019. agents. arXiv preprint arXiv:1811.01241, 2018. 5Preliminary code for downloading and using the small model Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "a1fb65be-8fb3-47a0-8cc2-4675cce557fa": {"__data__": {"id_": "a1fb65be-8fb3-47a0-8cc2-4675cce557fa", "embedding": null, "metadata": {"page_number": 0, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Tom B. Brown Benjamin Mann Nick Ryder Melanie Subbiah Jared Kaplan Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M. Ziegler Jeffrey Wu Clemens Winter2020 Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott GrayJul 22 Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI[cs.CL] Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ne-tuning on a specic task. While typically task-agnostic in architecture, this method still requires task-specic ne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art netuning approaches. Specically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ne-tuning, with tasks and few-shot demonstrations specied purely via text interaction with the model. GPT-3arXiv:2005.14165v4 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-y reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we nd that GPT-3 can generate samples of news articles which human evaluators have difculty distinguishing from articles written by humans. We discuss broader societal impacts of this nding and of GPT-3 in general. Equal contribution Johns Hopkins University, OpenAI", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "65c007a1-746b-4963-b52e-267363973a86": {"__data__": {"id_": "65c007a1-746b-4963-b52e-267363973a86", "embedding": null, "metadata": {"page_number": 1, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "1 Introduction 3 2 Approach 6 2.1 Model and Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Training Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3 Results 10 3.1 Language Modeling, Cloze, and Completion Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Closed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Winograd-Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.5 Common Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.6 Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.7 SuperGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.8 NLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.9 Synthetic and Qualitative Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4 Measuring and Preventing Memorization Of Benchmarks 29 5 Limitations 33 6 Broader Impacts 34 6.1 Misuse of Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2 Fairness, Bias, and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT-3 48 G Details of Task Phrasing and Specications 50 H Results on All Tasks for All Model Sizes 63", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "a1f72e0c-90d4-4dbe-b3ad-5ef3ca8733f5": {"__data__": {"id_": "a1f72e0c-90d4-4dbe-b3ad-5ef3ca8733f5", "embedding": null, "metadata": {"page_number": 2, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly exible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specic architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specic architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have been directly ne-tuned, entirely removing the need for task-specic architectures [RNSS18, DCLT18, HR18]. This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specic datasets and task-specic ne-tuning: to achieve strong performance on a desired task typically requires ne-tuning on a dataset of thousands to hundreds of thousands of examples specic to that task. Removing this limitation would be desirable, for several reasons. First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difcult to collect a large supervised training dataset, especially when the process must be repeated for every new task. Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus ne-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then ne-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specic to the training distribution and does not generalize well outside it [YdC+19, MPL19]. Thus, the performance of ne-tuned models on specic benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [GSL+18, NK19]. Third, humans do not require large supervised datasets to learn most language tasks a brief directive in natural language (e.g. please tell me if this sentence describes something happy or something sad) or at most a tiny number of demonstrations (e.g. here are two examples of people acting brave; please give a third example of bravery) is often Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in-context learning to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "53bfba63-f175-422b-90cd-33bbd61f0b18": {"__data__": {"id_": "53bfba63-f175-422b-90cd-33bbd61f0b18", "embedding": null, "metadata": {"page_number": 3, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper in-context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same uidity and generality. One potential route towards addressing these issues is meta-learning1 which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19] attempts to do this via what we call in-context learning, using the text input of a pretrained language model as a form of task specication: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next. While it has shown some initial promise, this approach still achieves results far inferior to ne-tuning for example [RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks. Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and nally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called zero-shot transfer, but this term is potentially ambiguous: the method is zero-shot in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term meta-learning to capture the inner-loop / outer-loop structure of the general method, and the term in context-learning to refer to the inner loop of meta-learning. We further specialize the description to zero-shot, one-shot, or few-shot depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training this is an important issue which we discuss later in the paper, but meta-learning is intended to encompass both possibilities, and simply describes the inner-outer loop structure.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "7f791e7d-d2cf-4d7c-9bb7-6599939fac91": {"__data__": {"id_": "7f791e7d-d2cf-4d7c-9bb7-6599939fac91", "embedding": null, "metadata": {"page_number": 4, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more procient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite. In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) few-shot learning, or in-context learning where we allow as many demonstrations as will t into the models context window (typically 10 to 100), (b) one-shot learning, where we allow only one demonstration, and (c) zero-shot learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional ne-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the models context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these learning curves involve no gradient updates or ne-tuning, just increasing numbers of demonstrations given as conditioning. Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by ne-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to ne-tuned models operating in the same closed-book setting. GPT-3 also displays one-shot and few-shot prociency at tasks designed to test rapid adaption or on-the-y reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them dened only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difculty distinguishing from human-generated articles. At the same time, we also nd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "f85fd716-a0da-4cf3-971f-b3761829a61f": {"__data__": {"id_": "f85fd716-a0da-4cf3-971f-b3761829a61f", "embedding": null, "metadata": {"page_number": 5, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we nd that data contamination has a minimal effect on GPT-3s performance on most datasets, we do identify a few datasets where it could be inating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity. In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we nd relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more procient meta-learners. Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3s characteristics in this regard. The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, oneand few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly dening and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specic data they tend to rely on. Specically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration): Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specic to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of ne-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In this work we do not ne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be ne-tuned in principle and this is a promising direction for future work. Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one nal example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100 as this is how many examples can t in the models context window (nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-specic data and reduced potential to learn an overly narrow distribution from a large but narrow ne-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art ne-tuned models. Also, a small amount of task specic data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML [HYC01, VBL+16] both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difcult to communicate the content or format of a task if no examples are given.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "dc3153ad-fe9b-41ef-8817-2538de4e1666": {"__data__": {"id_": "dc3153ad-fe9b-41ef-8817-2538de4e1666", "embedding": null, "metadata": {"page_number": 6, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "four methods for performing a task with a language model ne-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G. Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difcult for humans to understand the format of the task without prior examples, so this setting is in some cases unfairly hard. For example, if someone is asked to make a table of world records for the 200m dash, this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarication, understanding precisely what is desired can be difcult). Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction. Figure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specic benchmarks and sample efciency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art ne-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work. Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "b665eaeb-ebba-4ca7-9185-b3299bff7e14": {"__data__": {"id_": "b665eaeb-ebba-4ca7-9185-b3299bff7e14", "embedding": null, "metadata": {"page_number": 7, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "GPT-3 Small 125M 12 768 12 64 0.5M 6.0 104 GPT-3 Medium 350M 24 1024 16 64 0.5M 3.0 104 GPT-3 Large 760M 24 1536 16 96 0.5M 2.5 104 GPT-3 XL 1.3B 24 2048 24 128 1M 2.0 104 GPT-3 2.7B 2.7B 32 2560 32 80 1M 1.6 104 GPT-3 6.7B 6.7B 32 4096 32 128 2M 1.2 104 GPT-3 13B 13.0B 40 5140 40 128 2M 1.0 104 GPT-3 175B or GPT-3 175.0B 96 12288 96 128 3.2M 0.6 104 Table 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens. 2.1 Model and Architectures We use the same model and architecture as GPT-2 [RWC+19], including the modied initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks. Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters, nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, d= 4 dmodel), and dhead is the dimension of each attention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efciency and load-balancing in the layout of models across GPUs. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range. 2.2 Training Dataset Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2 [RSR+19] constituting nearly a trillion words. This size of dataset is sufcient to train our largest models without ever updating on the same sequence twice. However, we have found that unltered or lightly ltered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and ltered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overtting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity. Details of the rst two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected by scraping links over a longer period of time, and rst described in [KMH+20], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia. Table 2.2 shows the nal mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before ltering and 570GB after ltering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overtting in exchange for higher quality training data. 2https://commoncrawl.org/the-data/", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "208da89b-7a72-4927-8a85-115fa4c2e70a": {"__data__": {"id_": "208da89b-7a72-4927-8a85-115fa4c2e70a", "embedding": null, "metadata": {"page_number": 8, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "[KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D. Quantity Weight in Epochs elapsed when Dataset (tokens) training mix training for 300B tokens Common Crawl (ltered) 410 billion 60% 0.44 WebText2 19 billion 22% 2.9 Books1 12 billion 8% 1.9 Books2 55 billion 8% 0.43 Wikipedia 3 billion 3% 3.4 Table 2.2: Datasets used to train GPT-3. Weight in training mix refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once. A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the ltering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination. 2.3 Training Process As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPUs on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "04907dcb-1208-4e55-b364-76c2229467bf": {"__data__": {"id_": "04907dcb-1208-4e55-b364-76c2229467bf", "embedding": null, "metadata": {"page_number": 9, "file_name": "app/database/sample/02-gpt-3.pdf", "title": "GPT 3", "author": "Paul Graham"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that tasks training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it. K can be any value from 0 to the maximum amount allowed by the models context window, which is nctx = 2048 for all models and typically ts 10 to 100 examples. Larger values of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for K = 0, instead of) demonstrations. On tasks that involve choosing one correct completion from several options (multiple choice), we provide K examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benet as measured on the development set by normalizing by the unconditional probability of each completion, by computing P (completion|answerP (completion|context)context), where answer context is the string \"Answer: \" or \"A: \" and is used to prompt that the completion should be an answer but is otherwise generic. On tasks that involve binary classication, we give the options more semantically meaningful names (e.g. True or False rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR+19] (see Appendix G) for details. On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4 and a length penalty of = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to t on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else. 3 Results In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making efcient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks: tasks which require using the information stored in the models parameters to answer general knowledge questions. In Section 3.3 we evaluate the models ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the models performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we briey explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities these tasks focus on on-the-y reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}}, "docstore/metadata": {"43d84308-107b-4e14-ad67-4708ed0df562": {"doc_hash": "b2504d3aed6401107f1b8e695a69ab4199bc7bda3bb3b82149b9cd70ec36fe58"}, "0be1ad4b-d510-400f-b008-b9912accfd80": {"doc_hash": "1dbb3e67ae523c75a513cddf94ba3e9dce00f2d45a635bda539c52ba84d5e7f4"}, "5c2eb22f-2436-4825-9018-8c713245228b": {"doc_hash": "6b6a407ec203d994c8de049d9acc2e5598774e6cd6c0be5761fd1495a73c0574"}, "2f66606d-1cc9-4021-bd9d-dce8daec50dd": {"doc_hash": "32443ba9fe4554b750984cb2aea9c8003c71b6fe3558c64351dfa942442484bc"}, "5d78d132-a90b-439f-889c-e3ad51794a8e": {"doc_hash": "acd68ff5e0b5675b666173e9b31f9098fc1999328c76c97faa1fce7660d7228f"}, "278d4f3b-57f5-4afa-9e1e-3ce662b5687e": {"doc_hash": "aba4778f7776fc5e030d8f3053aa0a9b602d3e5ea9f7d4648f1578817710fa8f"}, "2109d277-d375-4186-b0ad-18c92b006c39": {"doc_hash": "64e9b40dfdf638957409a3e8ffb4834514a3cfa4e49b74568ad4f8a96a3d6e0e"}, "6882fc02-aded-461b-8609-0de660a75dc9": {"doc_hash": "33fe495fc5415a458d333461d2f61c116f682ed1a90856cf451d24f16746b4fe"}, "48f5221c-1ef3-4a07-b3ed-e440be9fae59": {"doc_hash": "8b3c81f52cec8dda597abaf58e1c9ed1843f6d9d222b3acec149d3df7e3bb23d"}, "00ea391a-a03f-43b5-8990-dbaf3dda8965": {"doc_hash": "908fb7af9e90a517ee144ed54992756943d8ed48628e271b6ed63f839c94202b"}, "baaee4f1-fa42-4f89-91be-2a5f740e48ae": {"doc_hash": "4d6f25031a292d49845f4d580a98e1a96644100b2058c7d52ac9cfa7617e45bd"}, "f1b2cbe9-1ac0-41d5-8793-d43a05ae0444": {"doc_hash": "85829b8680377a7c876cda9865220b16985e71ea42d2339d6dcae6a3f02badea"}, "b8a59434-fca7-4b7f-a4aa-38ac34fc1d8d": {"doc_hash": "115a95c8dbdcd56f1da27e8a02b3419faa70207cd0a462d5d3b260386aef60a2"}, "2081816a-3769-4ae2-89e5-7beccd142ab9": {"doc_hash": "5e5e81e7844fca0c4523377ab527873c75cd71437d41cf26f23a9745f999462f"}, "0826c996-847b-46e4-963e-abf51ae2df08": {"doc_hash": "ccee5611a9f99ad9b5f6838bd00499b3112dbbe7b384174df4f38a2245ae7482"}, "602481c4-6b4b-4649-9c8d-dbba6167cf29": {"doc_hash": "48dc0e6a5edb2921cbd2dfd6643e7118d3a55783c9033c0b4559dce8416c5210"}, "486adf83-a4c9-4d5c-8f8b-f1c483ac5894": {"doc_hash": "a6e0de040dd35d027b15f1b522dffd49493a91f9f199bf00a1bb82155ad1606e"}, "ed694ae6-7d75-461e-94e6-5ea880b4d67f": {"doc_hash": "4c5751e1402e7da53ffd144d7f3164f0135af6594b54c1de7b04888e2b399dd3"}, "d3fa1e96-f23f-4a61-bdb8-2e110988b99d": {"doc_hash": "ea7af240374f1d892aab31be336bb2fc287500a245f976b81c11e9e83b2c4ad6"}, "4fcf2312-cd07-4725-9e7c-5589bcf5a3b8": {"doc_hash": "f788dda64f0617faf0388f3fe693fa29996afa7e1e0ff53a1be8c5320f1e3a5a"}, "a1fb65be-8fb3-47a0-8cc2-4675cce557fa": {"doc_hash": "a705c4c85c1c5d6dcee7f8167f569c0abe82b281259fcb2c64ce4d892b8a9151"}, "65c007a1-746b-4963-b52e-267363973a86": {"doc_hash": "86938ab957fbe9bfcb2e4aed4c43d574af30092bb584a1a84d6a88fbf489dddd"}, "a1f72e0c-90d4-4dbe-b3ad-5ef3ca8733f5": {"doc_hash": "1730e632b8692dffad2af51c3b82483fd257c231f13f616d8cabc1a536547c98"}, "53bfba63-f175-422b-90cd-33bbd61f0b18": {"doc_hash": "22653d587e1a636b3c645ed7f1480b2517c62888556d0f8652bd24e388cfe633"}, "7f791e7d-d2cf-4d7c-9bb7-6599939fac91": {"doc_hash": "c5a25eae968cfe0f8fad2b7bf0b605ef4514e245f7425724c22b40c1cb5d115c"}, "f85fd716-a0da-4cf3-971f-b3761829a61f": {"doc_hash": "63627852231ddcc0977aaf0ca8c4f59881c5625d6503d2a036eac14933920801"}, "dc3153ad-fe9b-41ef-8817-2538de4e1666": {"doc_hash": "a373cd5f5c0bd165ff7f8fbfe8c3d16331257aab6cc6cfeacc1baee043fbd078"}, "b665eaeb-ebba-4ca7-9185-b3299bff7e14": {"doc_hash": "806d109cfb59594ee8012eade1b31a71b22a92a550805d2f56b4992a27b1981b"}, "208da89b-7a72-4927-8a85-115fa4c2e70a": {"doc_hash": "c0d0810c95d9432eb0b17fd6c0a9a9e7af40f1623e40cbbea8590ad924c2a198"}, "04907dcb-1208-4e55-b364-76c2229467bf": {"doc_hash": "2d8358245e4c263e036be4c1352a02e987bdbcbef11de40dfadc806300742884"}}}